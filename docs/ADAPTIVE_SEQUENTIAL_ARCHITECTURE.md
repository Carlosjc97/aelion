# Arquitectura Secuencial 2.0 ‚Äì Edaptia
Versi√≥n 20 de noviembre de 2025  
Responsable: Equipo Ara / Claude Code / T√∫

---

## 1. Por qu√© existe

Antes se intentaba generar TODO el curso en una sola llamada a OpenAI: 4‚Äë12 m√≥dulos + checkpoints + mock. Resultado: 3 minutos de espera, timeouts en Cloud Functions y usuarios frustrados.  
La nueva arquitectura divide el viaje en micro-tareas que se resuelven en paralelo, se cachean y se entregan al usuario en el momento exacto. As√≠ se obtiene feedback en <10‚ÄØs y el contenido pesado se cocina de fondo.

---

## 2. C√≥mo se siente para el usuario

1. **Generar plan con IA** ‚Üí inmediatamente aparece un skeleton con los slots de M1..Mx (5‚Äë10‚ÄØs).  
2. **M√≥dulo 1 se abre** ‚Üí la IA ya lo estaba generando; ves lecciones reales en ‚âà60‚ÄØs.  
3. **Mientras estudias**, el backend prepara M2. Cuando lo desbloqueas, ya est√° caliente.  
4. **Cada checkpoint** recalibra el plan y alimenta los siguientes m√≥dulos.  
5. **Todo el tiempo** recibes mensajes claros si algo falla (sin c√≥digos raros).

---

## 3. Flujo resumido

```mermaid
flowchart LR
    A[Quiz de colocaci√≥n] --> B{placementQuizGrade}
    B -->|band + learnerState| C[/adaptiveSession/start/]
    C --> D[adaptiveModuleCount]
    C --> E[adaptiveModuleGenerate (M1)]
    E --> F{Usuario abre M√≥dulo 1}
    F --> G[Prefetch M2]
    G --> H[Prefetch M3]
    F --> I[Checkpoint M1]
    I -->|new learnerState| G
```

- El front solo ve `/adaptiveSession/...`. Por dentro, Cloud Functions llama a los endpoints cl√°sicos (`adaptiveModuleCount`, `adaptiveModuleGenerate`, etc.) y persiste todo en Firestore.

---

## 4. Componentes clave

| Componente | Rol | Persistencia |
|------------|-----|--------------|
| `placementQuizStartLive` | Genera el quiz + guarda `quiz_session` | Firestore `quiz_sessions` |
| `placementQuizGrade` | Devuelve banda, score y `competencyMap` | Firestore `adaptive_sessions/{userId}` |
| `adaptiveSession/start` (wrapper nuevo) | Orquesta m√≥dulo count + prefetch de M1 | Firestore `adaptive_sessions` |
| `adaptiveModuleCount` | Devuelve `moduleCount` + `rationale` en 5‚Äë10‚ÄØs | Guardado en la sesi√≥n |
| `adaptiveModuleGenerate` | Genera un m√≥dulo completo (8‚Äë20 lecciones) | Cachea JSON en Storage + metadatos en Firestore |
| `adaptiveCheckpointQuiz` | Crea mini-quiz por m√≥dulo | Misma sesi√≥n |
| `adaptiveEvaluateCheckpoint` | Ajusta `learnerState` y desencadena el siguiente m√≥dulo | Firestore |
| Heath Scheduler (Cloud Function programada) | Prefetch + limpia sesiones viejas | Firestore / Storage |

> Nota: Los endpoints hist√≥ricos siguen existiendo, pero el cliente solo habla con `adaptiveSession/*`. Esto evita que un cambio en el front deje m√≥dulos a medio generar.

---

## 5. Estados y datos que se arrastran

| Campo | Fuente | Uso |
|-------|--------|-----|
| `band` | `placementQuizGrade` | Decide nivel inicial y temperatura de prompts |
| `learnerState` (skills 0‚Äë1) | Quiz + checkpoints | Alimenta prompts de m√≥dulos, boosters y checkpoints |
| `moduleCount` | `adaptiveModuleCount` | Skeleton UI + progress bar |
| `moduleStatus[n]` | Prefetch service | Informa si M1..Mx est√°n en `pending/generating/ready` |
| `focusSkills` | Checkpoint + heur√≠sticas | Inyectado en prompt de cada m√≥dulo |
| `locale/preferredLanguage` | Quiz + settings | Informa prompts y UI |

Cada vez que generamos algo, actualizamos este documento en Firestore. Cualquier funci√≥n puede reconstruir el contexto leyendo un √∫nico registro.

---

## 6. Estrategia ‚ÄúDisruptiva‚Äù (lo que entusiasma)

1. **Doble disparo al terminar el quiz**  
   - `adaptiveModuleCount` ‚Üí respuesta ligera para dibujar el plan.  
   - `adaptiveModuleGenerate` para M1 ‚Üí inicia de inmediato.  
   Esto ocurre en paralelo, sin esperar al usuario.

2. **Prefetch en cadena**  
   - Cuando M1 termina, Cloud Functions (Heath) lanza M2 en background.  
   - Si el usuario llega antes de que termine, se muestra el loader; si llega despu√©s, lo abre al instante.  
   - Cada checkpoint desencadena un ‚Äúprefetch + recalibraci√≥n‚Äù del siguiente.

3. **Cache inteligente**  
   - Respuestas de generaci√≥n se guardan en Storage con un hash del prompt.  
   - Si otro usuario pide ‚ÄúSQL para marketing ‚Äì banda basic‚Äù, reutilizamos resultados siempre que la ventana de frescura (24‚ÄØh) no haya expirado.

4. **Prompts con narrativa viva**  
   - Todos los prompts describen escenarios globales, invites reales (LATAM, startups, etc.).  
   - Se enviar√°n en ingl√©s para reducir coste y ganar consistencia; la traducci√≥n al usuario la hace el front (o el prompt incluye `language = es`).  
   - El doc de prompts vive en `functions/src/openai-service.ts` y se mantiene versionado.

5. **Mensajes humanos**  
   - ‚ÄúEstamos armando tu m√≥dulo con ejemplos de Growth en CDMX. Tardar√° ~1‚ÄØminuto.‚Äù  
   - ‚ÄúDetectamos que te cost√≥ `joins`. El siguiente m√≥dulo refuerza ese tema.‚Äù  
   Nada de errores JSON o `OPENAI_API_KEY not configured` en la UI.

---

## 7. Errores y c√≥mo reintentamos

| Falla | Acci√≥n autom√°tica | Mensaje al usuario |
|-------|------------------|--------------------|
| Timeout generando m√≥dulo | Reintenta hasta 3 veces con backoff | ‚ÄúEstamos tardando m√°s de lo normal, seguimos trabajando en tu m√≥dulo.‚Äù |
| 401 OpenAI | Cambia de key seg√∫n hint ‚Üí si persiste, alerta ops | ‚ÄúNecesitamos regenerar el contenido. Vuelve en 2 minutos.‚Äù |
| Cloud Firestore unavailable | Reintenta silenciosamente | ‚ÄúSincronizando tu progreso...‚Äù |
| Skeleton sin contenido >90‚ÄØs | Marca m√≥dulo como `error` y ofrece reintentar | ‚ÄúNo pudimos generar M2, ¬øreintentamos?‚Äù |

Todos los eventos cr√≠ticos se loguean en `openai_usage` con `endpoint`, `tokens`, `key_type`. Puedes monitorear en BigQuery.

---

## 8. Checklist para cualquier desarrollador nuevo

1. Leer `docs/Context_edaptia.md` ‚Üí visi√≥n general.  
2. Revisar este archivo para entender c√≥mo fluye el contenido.  
3. Explorar `functions/src/openai-service.ts` (prompts y l√≥gica).  
4. Ver `lib/features/quiz/quiz_screen.dart` ‚Üí `_bootstrap` maneja skeleton + m√≥dulos.  
5. Lanzar `firebase functions:log --only placementQuizStartLive` para confirmar que las keys est√°n vivas.  
6. Ejecutar `flutter run`, completar un quiz y verificar tiempos (<10‚ÄØs skeleton, <90‚ÄØs m√≥dulo).  
7. Si algo se rompe, actualizar el estado en `docs/RESUMEN_PARA_USUARIO.md`.

---

## 9. Roadmap de Implementaci√≥n (3 Fases)

### **Fase 1: MVP Secuencial (Semana 1 - P0 Critical)**
**Objetivo**: Skeleton en <10s, M1 en <90s. Usuario ve progreso inmediato.

**Tareas**:
1. ‚úÖ **Endpoints base ya existen**:
   - `adaptiveModuleCount` (retorna en 5-10s)
   - `adaptiveModuleGenerate` (genera m√≥dulo completo)
   - `placementQuizGrade` (calcula band + learnerState)

2. üî® **Crear wrapper `/adaptiveSession/start`** (`functions/src/index.ts`):
   ```typescript
   // Orquesta:
   // - Llamar adaptiveModuleCount (r√°pido)
   // - Inicializar moduleStatus en Firestore
   // - Disparar adaptiveModuleGenerate(M1) en background (NO esperar)
   // - Retornar skeleton inmediatamente
   ```

3. üî® **Agregar tracking de `moduleStatus` en Firestore**:
   ```typescript
   adaptive_sessions/{userId}/ {
     moduleStatus: {
       "1": "ready",      // Ya generado
       "2": "generating", // En proceso
       "3": "pending",    // No iniciado
       "4": "error"       // Fall√≥, reintentar
     }
   }
   ```

4. üî® **Flutter: UI skeleton + loader storytelling** (`lib/features/quiz/quiz_screen.dart`):
   - Mostrar estructura del curso (M1..Mx) en <10s
   - Loader para M1: "Armando tu m√≥dulo con ejemplos de [industria]. ~60s"
   - Polling cada 5s para actualizar `moduleStatus`
   - Si error, bot√≥n "Reintentar" que llama a `adaptiveModuleGenerate` nuevamente

**Entregables**:
- Usuario ve skeleton inmediatamente post-quiz
- M1 se genera en background, no bloquea UI
- Si tarda >90s, usuario ve mensaje claro (no timeout gen√©rico)

**Esfuerzo**: 4-5 d√≠as | **Prioridad**: P0

---

### **Fase 2: Prefetch Inteligente (Semana 2-3 - P1 High)**
**Objetivo**: M2 listo cuando usuario termina M1. Experiencia fluida sin esperas.

**Tareas**:
1. üî® **Prefetch con Firestore Triggers** (NO Cloud Scheduler inicialmente):
   ```typescript
   // functions/src/index.ts
   export const onModuleStatusChange = onDocumentWritten(
     "adaptive_sessions/{userId}",
     async (event) => {
       const moduleStatus = event.data.after.get("moduleStatus");

       // Si M1 === "ready" && M2 === "pending" && usuario abri√≥ M1
       if (userOpenedModule(1) && moduleStatus["2"] === "pending") {
         // Generar M2 en background
         await adaptiveModuleGenerateInternal({ moduleId: 2, ... });
       }
     }
   );
   ```

2. üî® **Condici√≥n cr√≠tica de engagement**:
   - **SOLO generar M+1 si usuario abri√≥ m√≥dulo anterior**
   - Rastrear `lastOpenedModule` en Firestore
   - Evitar precalentar cursos abandonados (ahorro de costos)

3. üî® **Recalibraci√≥n post-checkpoint**:
   - Actualizar `adaptiveEvaluateCheckpoint` para marcar siguiente m√≥dulo como "prefetching"
   - Trigger de Firestore pickea y regenera con nuevo `learnerState`

4. üî® **Timeout monitor** (Cloud Function programada cada 5 min):
   - Buscar m√≥dulos en "generating" por >10 minutos
   - Marcar como "error" y notificar ops
   - **Nota**: Esto S√ç requiere Cloud Scheduler, pero es opcional para MVP

**Entregables**:
- M2 listo cuando usuario completa M1
- Checkpoints ajustan contenido de m√≥dulos siguientes en tiempo real
- No se desperdician tokens OpenAI en cursos abandonados

**Esfuerzo**: 3-4 d√≠as | **Prioridad**: P1

---

### **Fase 3: Cache Sharing + Analytics (Semana 4+ - P2 Medium)**
**Objetivo**: Reducir costos 60-80% en contenido popular. M√©tricas para optimizar.

**Tareas**:
1. üî® **Storage caching layer**:
   ```typescript
   // Hash = SHA256(prompt + band + locale + PROMPT_VERSION)
   // Antes de llamar OpenAI:
   const cacheKey = `gs://aelion-cache/${hash}.json`;
   const cached = await storage.bucket().file(cacheKey).exists();
   if (cached && createdAt < 24h) return cached;

   // Guardar + timestamp
   await storage.bucket().file(cacheKey).save(result);
   ```

2. üî® **Prompt versioning**:
   - Incluir `PROMPT_VERSION = "2025-11-21"` en hash
   - Invalidar cache autom√°ticamente cuando se actualicen prompts

3. üî® **M√©tricas en `openai_usage`**:
   - Agregar campo `cache_hit: boolean`
   - Dashboard BigQuery: % cache hit rate, tokens ahorrados

4. üî® **Analytics dashboard**:
   - Tiempo hasta skeleton (<10s ‚úÖ)
   - Tiempo hasta M1 ready (<90s ‚úÖ)
   - % sesiones donde M2 ready antes de que usuario llegue
   - Tasa de abandono por m√≥dulo

**Entregables**:
- M√≥dulos populares (SQL b√°sico, Ingl√©s A1) se reutilizan entre usuarios
- Dashboard para monitorear performance
- Costos OpenAI reducidos dr√°sticamente

**Esfuerzo**: 5-6 d√≠as | **Prioridad**: P2 (solo si ya hay >100 usuarios/d√≠a)

---

## 10. Decisiones de Arquitectura Clave

### **¬øCloud Scheduler o Firestore Triggers?**
**Decisi√≥n**: Empezar con **Firestore Triggers** para prefetch.

**Razones**:
- Cloud Scheduler = costo adicional (free tier solo 3 jobs)
- Triggers reaccionan en tiempo real a cambios de estado (m√°s eficiente)
- Si escala mal, migrar a Scheduler en Fase 3

**Excepci√≥n**: Timeout monitor S√ç usa Cloud Scheduler (polling cada 5 min).

### **¬øPrefetch de cu√°ntos m√≥dulos?**
**Decisi√≥n**: Solo **M+1** (siguiente m√≥dulo).

**Razones**:
- Balance costo/beneficio √≥ptimo
- Reduce tokens desperdiciados en cursos abandonados
- Si usuario vuela los m√≥dulos, el trigger de Firestore mantiene M+2 cerca

### **¬øCache desde MVP?**
**Decisi√≥n**: **NO**. Cache solo en Fase 3.

**Razones**:
- Complejidad innecesaria para <100 usuarios
- Riesgo de stale content si bugs en prompts
- M√©tricas primero, optimizaci√≥n despu√©s

---

## 11. Checklist de Integraci√≥n

Antes de marcar cada fase como "completa", verificar:

**Fase 1**:
- [ ] `/adaptiveSession/start` retorna en <10s
- [ ] Skeleton UI se muestra inmediatamente post-quiz
- [ ] M1 aparece en <90s desde inicio de generaci√≥n
- [ ] Loader muestra mensaje storytelling (no "Loading...")
- [ ] Error messages son humanos ("Reintentando...", no "500 Internal Server Error")

**Fase 2**:
- [ ] M2 inicia generaci√≥n solo si usuario abri√≥ M1
- [ ] Checkpoint recalibra `learnerState` y regenera siguiente m√≥dulo
- [ ] No hay m√≥dulos "stuck" en "generating" >10 min (timeout monitor)
- [ ] Logs muestran `prefetch triggered by user engagement`

**Fase 3**:
- [ ] Cache hit rate >60% para m√≥dulos populares
- [ ] Dashboard muestra "Time to skeleton" promedio <10s
- [ ] Tokens OpenAI consumidos bajaron 50%+ mes a mes
- [ ] Prompt updates invalidan cache (no stale content)

---

La meta es que cualquier persona que abra esta app diga: *"Wow, se nota que me est√°n creando algo a medida y lo hacen r√°pido."* Si en alg√∫n paso no se siente as√≠, vuelves a este documento, detectas el cuello de botella y lo resuelves. No m√°s monolitos lentos. #VamosPorEl10/10
+++++